{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b35c00",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Vergelijking van de modellen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_PATH = \"../data/Chats/\"\n",
    "OUTPUT_FILE = \"samengevoegd_met_samenvattingen_local.jsonl\"\n",
    "MODEL_NAME = \"philschmid/bart-large-cnn-samsum\"  # Goed voor dialoogachtige samenvattingen\n",
    "\n",
    "# === SAMENVATTINGSMODEL LADEN ===\n",
    "summarizer = pipeline(\"summarization\", model=MODEL_NAME, tokenizer=MODEL_NAME, device=0)  # device=0 voor GPU\n",
    "\n",
    "# === BESTANDEN VERWERKEN ===\n",
    "all_chat_data = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(DATA_PATH, \"*.json\")):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    topics = defaultdict(list)\n",
    "    for item in data:\n",
    "        topics[item[\"topic_id\"]].append(item[\"text\"])\n",
    "\n",
    "    for topic_id, messages in topics.items():\n",
    "        combined_text = \" \".join(messages)\n",
    "        if len(combined_text) < 50:\n",
    "            continue  # te kort om te verwerken\n",
    "\n",
    "        # Samenvatting genereren\n",
    "        try:\n",
    "            summary = summarizer(combined_text, max_length=100, min_length=30, do_sample=False)[0][\"summary_text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Fout bij samenvatten: {e}\")\n",
    "            summary = \"Samenvatting niet beschikbaar.\"\n",
    "\n",
    "        all_chat_data.append({\n",
    "            \"topic_id\": topic_id,\n",
    "            \"chat\": combined_text,\n",
    "            \"summary\": summary\n",
    "        })\n",
    "\n",
    "# === OPSLAAN ALS JSONL ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in all_chat_data:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Dataset opgeslagen als {OUTPUT_FILE} met {len(all_chat_data)} items.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85817f29",
   "metadata": {},
   "source": [
    "## Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f61444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_8504\\4089401470.py:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  with open(\"..\\data\\Chats\\chatlog_topic_001_20250521_011132.json\", \"r\", encoding=\"utf-8\") as f:\n",
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 21.81 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_8504\\4089401470.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart-summarizer\\\\tokenizer_config.json',\n",
       " './bart-summarizer\\\\special_tokens_map.json',\n",
       " './bart-summarizer\\\\vocab.json',\n",
       " './bart-summarizer\\\\merges.txt',\n",
       " './bart-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Dataset laden of creëren\n",
    "# Stap 1: JSONL of JSON-bestand laden\n",
    "with open(\"..\\data\\Chats\\chatlog_topic_001_20250521_011132.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)  # of gebruik jsonlines als het .jsonl is\n",
    "\n",
    "# Stap 2: Groeperen per topic_id\n",
    "topics = defaultdict(list)\n",
    "for item in data:\n",
    "    topics[item[\"topic_id\"]].append(item[\"text\"])\n",
    "\n",
    "# Stap 3: Conversaties samenvoegen\n",
    "chat_data = []\n",
    "for topic_id, messages in topics.items():\n",
    "    combined_text = \" \".join(messages)\n",
    "    chat_data.append({\n",
    "        \"chat\": combined_text,\n",
    "        \"summary\": \"De deelnemers bespreken het plan om windmolens net buiten het dorp te plaatsen. Er is brede instemming dat meer informatie nodig is voordat een oordeel geveld kan worden. Belangrijke zorgen zijn onder andere de impact op het landschap, mogelijke geluidsoverlast en de kosten. Iedereen benadrukt dat alle aspecten zorgvuldig moeten worden afgewogen voordat er een beslissing wordt genomen.\"\n",
    "    })\n",
    "\n",
    "# Stap 4: Dataset aanmaken\n",
    "dataset = Dataset.from_list(chat_data)\n",
    "\n",
    "# Tokenizer en model laden\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"chat\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./bart-summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76adb7",
   "metadata": {},
   "source": [
    "## T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840ff31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 98.25 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_8504\\1563979243.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:06, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./t5-summarizer\\\\tokenizer_config.json',\n",
       " './t5-summarizer\\\\special_tokens_map.json',\n",
       " './t5-summarizer\\\\spiece.model',\n",
       " './t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63a0e6",
   "metadata": {},
   "source": [
    "## T5-Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f4acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-tglobal-base and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 99.58 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_8504\\1979014229.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 07:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./long-t5-summarizer\\\\tokenizer_config.json',\n",
       " './long-t5-summarizer\\\\special_tokens_map.json',\n",
       " './long-t5-summarizer\\\\spiece.model',\n",
       " './long-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=4096, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./long-t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./long-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./long-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7aaf0",
   "metadata": {},
   "source": [
    "## T5-Flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b28ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\caspe\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 14.07 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_8504\\195943.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:19, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./flan-t5-summarizer\\\\tokenizer_config.json',\n",
       " './flan-t5-summarizer\\\\special_tokens_map.json',\n",
       " './flan-t5-summarizer\\\\spiece.model',\n",
       " './flan-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./flan-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796db99a",
   "metadata": {},
   "source": [
    "## PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b220004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 325.52 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_8504\\542700176.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./pegasus-summarizer\\\\tokenizer_config.json',\n",
       " './pegasus-summarizer\\\\special_tokens_map.json',\n",
       " './pegasus-summarizer\\\\spiece.model',\n",
       " './pegasus-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pegasus-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./pegasus-summarizer\")\n",
    "tokenizer.save_pretrained(\"./pegasus-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f83064",
   "metadata": {},
   "source": [
    "## LED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0720e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1/1 [00:00<00:00, 12.22 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_8504\\3721482775.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Input ids are automatically padded from 512 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./longformer-summarizer\\\\tokenizer_config.json',\n",
       " './longformer-summarizer\\\\special_tokens_map.json',\n",
       " './longformer-summarizer\\\\vocab.json',\n",
       " './longformer-summarizer\\\\merges.txt',\n",
       " './longformer-summarizer\\\\added_tokens.json',\n",
       " './longformer-summarizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./longformer-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./longformer-summarizer\")\n",
    "tokenizer.save_pretrained(\"./longformer-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b33141",
   "metadata": {},
   "source": [
    "## Lammla ----> QWen\n",
    "\n",
    "Dataset moet er anders uit zien met promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\caspe\\.cache\\huggingface\\hub\\models--openlm-research--open_llama_7b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "You are using a model of type llama to instantiate a model of type open-llama. This is not supported for all configurations of models and can yield errors.\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# Tokenizer en model (Qwen)\n",
    "model_name = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./qwen-summarizer\")\n",
    "tokenizer.save_pretrained(\"./qwen-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dae39a",
   "metadata": {},
   "source": [
    "## XLNet ---> Niet meer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f513b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\caspe\\.cache\\huggingface\\hub\\models--xlnet--xlnet-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 238.15 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_20916\\1113697980.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: last_hidden_state. For reference, the inputs it received are input_ids,token_type_ids,attention_mask,labels.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     46\u001b[39m trainer = Trainer(\n\u001b[32m     47\u001b[39m     model=model,\n\u001b[32m     48\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     data_collator=data_collator\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Training starten\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Model opslaan\u001b[39;00m\n\u001b[32m     58\u001b[39m model.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./xlnet-summarizer\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3822\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[32m-> \u001b[39m\u001b[32m3822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3823\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3824\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(outputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3825\u001b[39m         )\n\u001b[32m   3826\u001b[39m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[32m   3827\u001b[39m     loss = outputs[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: The model did not return a loss from the inputs, only the following keys: last_hidden_state. For reference, the inputs it received are input_ids,token_type_ids,attention_mask,labels."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLNetModel\n",
    "\n",
    "# Voorbeeld dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"chat\": [\n",
    "        \"Hoi, hoe is het met je? Ik had gisteren een drukke dag op werk.\",\n",
    "        \"Kan je me helpen met het instellen van mijn router?\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Persoon praat over een drukke werkdag.\",\n",
    "        \"Persoon vraagt hulp bij instellen van een router.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"xlnet/xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = XLNetModel.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlnet-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./xlnet-summarizer\")\n",
    "tokenizer.save_pretrained(\"./xlnet-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a3c222",
   "metadata": {},
   "source": [
    "## Minstral\n",
    "\n",
    "Dataset moet er anders uit zien met promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # vaak nodig bij decoder-only modellen\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Mistral-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./Mistral-summarizer\")\n",
    "tokenizer.save_pretrained(\"./Mistral-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7f751",
   "metadata": {},
   "source": [
    "## Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97088bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# Tokenizer en model (Qwen)\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./gemma-summarizer\")\n",
    "tokenizer.save_pretrained(\"./gemma-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bf453",
   "metadata": {},
   "source": [
    "## OpenChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "model_name = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./openchat-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./openchat-summarizer\")\n",
    "tokenizer.save_pretrained(\"./openchat-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f5169",
   "metadata": {},
   "source": [
    "# Evaluatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82efed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    \"chat\": [\n",
    "        \"Hoi allemaal! Ze willen windmolens bouwen buiten het dorp. Wat vinden jullie? Er zijn zorgen over geluidsoverlast en impact op het landschap.\",\n",
    "        \"We hebben een bijeenkomst gepland over het nieuwe buurthuis. Mensen willen weten of er genoeg budget is en hoe de planning eruitziet.\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Er is discussie over de bouw van windmolens buiten het dorp, met zorgen over geluid en landschap.\",\n",
    "        \"Er komt een bijeenkomst over het buurthuis, met vragen over budget en planning.\"\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6af9cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./bart-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 64, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 64, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./long-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 64, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./flan-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./longformer-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 64, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Input ids are automatically padded from 52 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Your max_length is set to 64, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Input ids are automatically padded from 51 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./pegasus-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Your max_length is set to 64, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 64, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ROUGE Vergelijking ===\n",
      "                  model  rouge1  rouge2  rougeL  rougeLsum\n",
      "      ./bart-summarizer  0.4879  0.1417  0.4337     0.4337\n",
      "   ./long-t5-summarizer  0.0000  0.0000  0.0000     0.0000\n",
      "   ./flan-t5-summarizer  0.3749  0.0816  0.3463     0.3463\n",
      "./longformer-summarizer  0.4808  0.1399  0.4274     0.4274\n",
      "        ./t5-summarizer  0.4647  0.0982  0.4020     0.4020\n",
      "   ./pegasus-summarizer  0.0816  0.0213  0.0816     0.0816\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = []\n",
    "model_paths = [\n",
    "    \"./bart-summarizer\",\n",
    "    \"./long-t5-summarizer\",\n",
    "    \"./flan-t5-summarizer\",\n",
    "    \"./longformer-summarizer\",\n",
    "    \"./t5-summarizer\",\n",
    "    \"./pegasus-summarizer\",\n",
    "]\n",
    "\n",
    "for model_path in model_paths:\n",
    "    print(f\"\\nEvaluating: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    predictions = [\n",
    "        summarizer(text, max_length=64, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "        for text in test_data[\"chat\"]\n",
    "    ]\n",
    "\n",
    "    scores = rouge.compute(predictions=predictions, references=test_data[\"summary\"])\n",
    "    results.append({\n",
    "        \"model\": model_path,\n",
    "        \"rouge1\": round(scores[\"rouge1\"], 4),\n",
    "        \"rouge2\": round(scores[\"rouge2\"], 4),\n",
    "        \"rougeL\": round(scores[\"rougeL\"], 4),\n",
    "        \"rougeLsum\": round(scores[\"rougeLsum\"], 4)\n",
    "    })\n",
    "\n",
    "# Tabel tonen\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n=== ROUGE Vergelijking ===\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64bb80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./bart-summarizer</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./long-t5-summarizer</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./flan-t5-summarizer</td>\n",
       "      <td>0.3749</td>\n",
       "      <td>0.0816</td>\n",
       "      <td>0.3463</td>\n",
       "      <td>0.3463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./longformer-summarizer</td>\n",
       "      <td>0.4808</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.4274</td>\n",
       "      <td>0.4274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./t5-summarizer</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.0982</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>0.4020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./pegasus-summarizer</td>\n",
       "      <td>0.0816</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>0.0816</td>\n",
       "      <td>0.0816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  rouge1  rouge2  rougeL  rougeLsum\n",
       "0        ./bart-summarizer  0.4879  0.1417  0.4337     0.4337\n",
       "1     ./long-t5-summarizer  0.0000  0.0000  0.0000     0.0000\n",
       "2     ./flan-t5-summarizer  0.3749  0.0816  0.3463     0.3463\n",
       "3  ./longformer-summarizer  0.4808  0.1399  0.4274     0.4274\n",
       "4          ./t5-summarizer  0.4647  0.0982  0.4020     0.4020\n",
       "5     ./pegasus-summarizer  0.0816  0.0213  0.0816     0.0816"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5182de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
