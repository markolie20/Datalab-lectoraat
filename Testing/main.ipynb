{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b35c00",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Vergelijking van de modellen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b60a0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10872 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_002_20250605_110618.json\n",
      "165 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_003_20250605_111208.json\n",
      "170 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_004_20250605_111802.json\n",
      "166 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_005_20250605_112316.json\n",
      "172 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_006_20250605_112912.json\n",
      "161 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_007_20250605_113423.json\n",
      "164 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_008_20250605_114010.json\n",
      "165 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_009_20250605_114525.json\n",
      "173 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_010_20250605_121148.json\n",
      "✅ Dataset opgeslagen als samengevoegd_met_samenvattingen_local.jsonl met 9 items.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_PATH = r\"C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\"\n",
    "OUTPUT_FILE = \"samengevoegd_met_samenvattingen_local.jsonl\"\n",
    "MODEL_NAME = \"philschmid/bart-large-cnn-samsum\"\n",
    "\n",
    "# === SAMENVATTINGSMODEL EN TOKENIZER LADEN ===\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=MODEL_NAME, tokenizer=MODEL_NAME, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_INPUT_LENGTH = tokenizer.model_max_length\n",
    "\n",
    "def split_text_by_tokens(text, tokenizer, max_input_tokens):\n",
    "    \"\"\"\n",
    "    Splitst een lange tekst in stukken die elk maximaal max_input_tokens tokens bevatten.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_input_tokens):\n",
    "        chunk_tokens = tokens[i:i+max_input_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# === BESTANDEN VERWERKEN ===\n",
    "all_chat_data = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(DATA_PATH, \"*.json\")):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        print(len(data), \"items gevonden in\", filepath)\n",
    "    topics = defaultdict(list)\n",
    "    for item in data:\n",
    "        topics[item[\"topic_id\"]].append(item[\"text\"])\n",
    "\n",
    "    for topic_id, messages in topics.items():\n",
    "        combined_text = \" \".join(messages)\n",
    "        if len(combined_text.strip()) < 50:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text_chunks = split_text_by_tokens(combined_text, tokenizer, MAX_INPUT_LENGTH - 5)\n",
    "            summaries = []\n",
    "\n",
    "            for chunk in text_chunks:\n",
    "                if not chunk.strip():\n",
    "                    continue\n",
    "                summary_output = summarizer(\n",
    "                    chunk,\n",
    "                    max_length=100,\n",
    "                    min_length=30,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "                summaries.append(summary_output[0][\"summary_text\"])\n",
    "\n",
    "            full_summary = \" \".join(summaries)\n",
    "            if not full_summary.strip():\n",
    "                full_summary = \"No summary generated.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing topic_id {topic_id}: {e}\")\n",
    "            full_summary = \"Summary not available due to error.\"\n",
    "\n",
    "        all_chat_data.append({\n",
    "            \"topic_id\": topic_id,\n",
    "            \"chat\": combined_text,\n",
    "            \"summary\": full_summary\n",
    "        })\n",
    "\n",
    "# === OPSLAAN ALS JSONL ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in all_chat_data:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Dataset opgeslagen als {OUTPUT_FILE} met {len(all_chat_data)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ab5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map:   0%|          | 0/9 [00:00<?, ? examples/s]c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 14.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Laad je jsonl-bestand in als een lijst van dicts\n",
    "with open(\"samengevoegd_met_samenvattingen_local.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chat_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Maak een Huggingface Dataset\n",
    "dataset = Dataset.from_list(chat_data)\n",
    "# Tokenizer en model laden\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"chat\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85817f29",
   "metadata": {},
   "source": [
    "## Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f61444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 15.37 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_28992\\3881543212.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart-summarizer\\\\tokenizer_config.json',\n",
       " './bart-summarizer\\\\special_tokens_map.json',\n",
       " './bart-summarizer\\\\vocab.json',\n",
       " './bart-summarizer\\\\merges.txt',\n",
       " './bart-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer en model laden\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"chat\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./bart-summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76adb7",
   "metadata": {},
   "source": [
    "## T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ff31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 39.79 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_16024\\1563979243.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.419400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./t5-summarizer\\\\tokenizer_config.json',\n",
       " './t5-summarizer\\\\special_tokens_map.json',\n",
       " './t5-summarizer\\\\spiece.model',\n",
       " './t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63a0e6",
   "metadata": {},
   "source": [
    "## T5-Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f4acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-tglobal-base and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 29.91 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_11024\\2989404296.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:06, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>26.186200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./long-t5-summarizer\\\\tokenizer_config.json',\n",
       " './long-t5-summarizer\\\\special_tokens_map.json',\n",
       " './long-t5-summarizer\\\\spiece.model',\n",
       " './long-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./long-t5-summarizer\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,    \n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./long-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./long-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d2a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from torchvision) (2.2.5)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\caspe\\jaar 3\\datalab\\git\\datalab-lectoraat\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp312-cp312-win_amd64.whl (2817.2 MB)\n",
      "   ---------------------------------------- 0.0/2.8 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.8 GB 46.3 MB/s eta 0:01:01\n",
      "   ---------------------------------------- 0.0/2.8 GB 53.8 MB/s eta 0:00:52\n",
      "    --------------------------------------- 0.0/2.8 GB 65.4 MB/s eta 0:00:43\n",
      "    --------------------------------------- 0.1/2.8 GB 73.6 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 0.1/2.8 GB 73.1 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 0.1/2.8 GB 73.5 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 0.1/2.8 GB 75.5 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 0.1/2.8 GB 75.8 MB/s eta 0:00:36\n",
      "   -- ------------------------------------- 0.1/2.8 GB 78.0 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 0.2/2.8 GB 80.1 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 0.2/2.8 GB 81.6 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 0.2/2.8 GB 81.5 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 0.2/2.8 GB 83.1 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 0.2/2.8 GB 84.2 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 0.3/2.8 GB 86.8 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 90.1 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 90.1 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 87.7 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 85.9 MB/s eta 0:00:29\n",
      "   ---- ----------------------------------- 0.3/2.8 GB 86.8 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 85.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 85.9 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 85.5 MB/s eta 0:00:29\n",
      "   ----- ---------------------------------- 0.4/2.8 GB 85.1 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.4/2.8 GB 83.8 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.4/2.8 GB 83.4 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.5/2.8 GB 83.4 MB/s eta 0:00:29\n",
      "   ------ --------------------------------- 0.5/2.8 GB 82.1 MB/s eta 0:00:29\n",
      "   ------- -------------------------------- 0.5/2.8 GB 83.0 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 82.6 MB/s eta 0:00:28\n",
      "   ------- -------------------------------- 0.5/2.8 GB 83.4 MB/s eta 0:00:28\n",
      "   -------- ------------------------------- 0.6/2.8 GB 85.5 MB/s eta 0:00:27\n",
      "   -------- ------------------------------- 0.6/2.8 GB 88.2 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 0.6/2.8 GB 88.2 MB/s eta 0:00:26\n",
      "   -------- ------------------------------- 0.6/2.8 GB 90.6 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 0.6/2.8 GB 91.6 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 0.7/2.8 GB 92.1 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 0.7/2.8 GB 92.6 MB/s eta 0:00:24\n",
      "   --------- ------------------------------ 0.7/2.8 GB 90.6 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 90.6 MB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 0.7/2.8 GB 92.1 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 92.6 MB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 0.8/2.8 GB 93.1 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 93.1 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 92.1 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 90.6 MB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 0.8/2.8 GB 91.1 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 0.9/2.8 GB 91.6 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 0.9/2.8 GB 92.1 MB/s eta 0:00:21\n",
      "   ------------ --------------------------- 0.9/2.8 GB 89.2 MB/s eta 0:00:22\n",
      "   ------------ --------------------------- 0.9/2.8 GB 85.5 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 0.9/2.8 GB 85.9 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 0.9/2.8 GB 88.7 MB/s eta 0:00:22\n",
      "   ------------- -------------------------- 1.0/2.8 GB 89.6 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 90.1 MB/s eta 0:00:21\n",
      "   -------------- ------------------------- 1.0/2.8 GB 90.6 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 1.0/2.8 GB 91.1 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 1.1/2.8 GB 92.1 MB/s eta 0:00:20\n",
      "   --------------- ------------------------ 1.1/2.8 GB 94.2 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 1.1/2.8 GB 94.2 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 1.1/2.8 GB 94.2 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 1.1/2.8 GB 93.1 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 101.0 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 101.6 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 1.2/2.8 GB 102.2 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 102.8 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 1.2/2.8 GB 104.8 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 1.3/2.8 GB 101.6 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 1.3/2.8 GB 101.0 MB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.3/2.8 GB 99.8 MB/s eta 0:00:16\n",
      "   ------------------ --------------------- 1.3/2.8 GB 101.0 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 1.3/2.8 GB 98.6 MB/s eta 0:00:16\n",
      "   ------------------- -------------------- 1.4/2.8 GB 99.8 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.8 GB 102.8 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 1.4/2.8 GB 101.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 1.4/2.8 GB 98.0 MB/s eta 0:00:15\n",
      "   -------------------- ------------------- 1.4/2.8 GB 100.4 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 1.5/2.8 GB 99.8 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 1.5/2.8 GB 99.2 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 1.5/2.8 GB 99.2 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 1.5/2.8 GB 97.5 MB/s eta 0:00:14\n",
      "   --------------------- ------------------ 1.5/2.8 GB 100.3 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 101.0 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 101.6 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 1.6/2.8 GB 101.0 MB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 100.4 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 1.6/2.8 GB 102.8 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 101.0 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 1.7/2.8 GB 101.6 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 1.7/2.8 GB 102.2 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 101.0 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 1.7/2.8 GB 101.6 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 1.8/2.8 GB 101.5 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 1.8/2.8 GB 102.2 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 1.8/2.8 GB 101.6 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 1.8/2.8 GB 104.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 1.9/2.8 GB 103.5 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 1.9/2.8 GB 105.4 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 1.9/2.8 GB 102.2 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 1.9/2.8 GB 106.1 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 1.9/2.8 GB 103.5 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 2.0/2.8 GB 102.8 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 106.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 103.4 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 101.6 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 2.0/2.8 GB 101.0 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 101.5 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 101.0 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 2.1/2.8 GB 101.0 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 2.1/2.8 GB 101.0 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.1/2.8 GB 98.6 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.2/2.8 GB 101.0 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 2.2/2.8 GB 98.6 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 98.0 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 96.3 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 93.1 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 2.2/2.8 GB 93.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.3/2.8 GB 91.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.3/2.8 GB 91.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.3/2.8 GB 86.9 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.3/2.8 GB 84.7 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 2.3/2.8 GB 83.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 2.3/2.8 GB 84.6 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 2.3/2.8 GB 81.8 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 2.4/2.8 GB 80.6 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 2.4/2.8 GB 79.4 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 2.4/2.8 GB 77.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 76.5 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 75.2 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 2.4/2.8 GB 74.5 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 2.5/2.8 GB 73.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 73.8 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 78.3 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 2.5/2.8 GB 79.8 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.5/2.8 GB 80.6 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.6/2.8 GB 83.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 2.6/2.8 GB 85.5 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.6/2.8 GB 86.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.6/2.8 GB 87.7 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.6/2.8 GB 89.2 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 2.6/2.8 GB 88.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.7/2.8 GB 88.7 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 89.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 90.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 91.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 90.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.7/2.8 GB 89.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 85.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 81.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 80.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 78.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 76.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 76.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.8/2.8 GB 75.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 GB 10.8 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp312-cp312-win_amd64.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 47.5 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.1/4.1 MB 34.8 MB/s eta 0:00:00\n",
      "Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Installing collected packages: pillow, torch, torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/4 [pillow]\n",
      "   ---------------------------------------- 0/4 [pillow]\n",
      "   ---------------------------------------- 0/4 [pillow]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   ---------- ----------------------------- 1/4 [torch]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   -------------------- ------------------- 2/4 [torchvision]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ------------------------------ --------- 3/4 [torchaudio]\n",
      "   ---------------------------------------- 4/4 [torchaudio]\n",
      "\n",
      "Successfully installed pillow-11.0.0 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7aaf0",
   "metadata": {},
   "source": [
    "## T5-Flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b28ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 37.66 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_34044\\4066598123.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./flan-t5-summarizer\\\\tokenizer_config.json',\n",
       " './flan-t5-summarizer\\\\special_tokens_map.json',\n",
       " './flan-t5-summarizer\\\\spiece.model',\n",
       " './flan-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./flan-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796db99a",
   "metadata": {},
   "source": [
    "## PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b220004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 38.07 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_9712\\1184256781.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 08:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.573100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./pegasus-summarizer\\\\tokenizer_config.json',\n",
       " './pegasus-summarizer\\\\special_tokens_map.json',\n",
       " './pegasus-summarizer\\\\spiece.model',\n",
       " './pegasus-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pegasus-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./pegasus-summarizer\")\n",
    "tokenizer.save_pretrained(\"./pegasus-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f83064",
   "metadata": {},
   "source": [
    "## LED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0720e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 32.72 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_9712\\3891717521.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Input ids are automatically padded from 512 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.553600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./longformer-summarizer\\\\tokenizer_config.json',\n",
       " './longformer-summarizer\\\\special_tokens_map.json',\n",
       " './longformer-summarizer\\\\vocab.json',\n",
       " './longformer-summarizer\\\\merges.txt',\n",
       " './longformer-summarizer\\\\added_tokens.json',\n",
       " './longformer-summarizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./longformer-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./longformer-summarizer\")\n",
    "tokenizer.save_pretrained(\"./longformer-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b33141",
   "metadata": {},
   "source": [
    "## Lammla ----> QWen\n",
    "\n",
    "Dataset moet er anders uit zien met promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb26aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\caspe\\.cache\\huggingface\\hub\\models--openlm-research--open_llama_7b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "You are using a model of type llama to instantiate a model of type open-llama. This is not supported for all configurations of models and can yield errors.\n",
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# Tokenizer en model (Qwen)\n",
    "model_name = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./qwen-summarizer\")\n",
    "tokenizer.save_pretrained(\"./qwen-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dae39a",
   "metadata": {},
   "source": [
    "## XLNet ---> Niet meer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f513b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\caspe\\.cache\\huggingface\\hub\\models--xlnet--xlnet-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 238.15 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_20916\\1113697980.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: last_hidden_state. For reference, the inputs it received are input_ids,token_type_ids,attention_mask,labels.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     46\u001b[39m trainer = Trainer(\n\u001b[32m     47\u001b[39m     model=model,\n\u001b[32m     48\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     data_collator=data_collator\n\u001b[32m     52\u001b[39m )\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Training starten\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Model opslaan\u001b[39;00m\n\u001b[32m     58\u001b[39m model.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./xlnet-summarizer\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3822\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[32m-> \u001b[39m\u001b[32m3822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3823\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3824\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(outputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3825\u001b[39m         )\n\u001b[32m   3826\u001b[39m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[32m   3827\u001b[39m     loss = outputs[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: The model did not return a loss from the inputs, only the following keys: last_hidden_state. For reference, the inputs it received are input_ids,token_type_ids,attention_mask,labels."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLNetModel\n",
    "\n",
    "# Voorbeeld dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    \"chat\": [\n",
    "        \"Hoi, hoe is het met je? Ik had gisteren een drukke dag op werk.\",\n",
    "        \"Kan je me helpen met het instellen van mijn router?\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Persoon praat over een drukke werkdag.\",\n",
    "        \"Persoon vraagt hulp bij instellen van een router.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"xlnet/xlnet-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = XLNetModel.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlnet-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./xlnet-summarizer\")\n",
    "tokenizer.save_pretrained(\"./xlnet-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a3c222",
   "metadata": {},
   "source": [
    "## Minstral\n",
    "\n",
    "Dataset moet er anders uit zien met promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bbada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # vaak nodig bij decoder-only modellen\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Mistral-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./Mistral-summarizer\")\n",
    "tokenizer.save_pretrained(\"./Mistral-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7f751",
   "metadata": {},
   "source": [
    "## Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97088bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# Tokenizer en model (Qwen)\n",
    "model_name = \"google/gemma-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./gemma-summarizer\")\n",
    "tokenizer.save_pretrained(\"./gemma-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2bf453",
   "metadata": {},
   "source": [
    "## OpenChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset laden\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\")[\"train\"]\n",
    "\n",
    "# Combineer instructie, input, en output\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"\"\"### Instructie:\n",
    "{example['instruction']}\n",
    "\n",
    "### Invoer:\n",
    "{example['input']}\n",
    "\n",
    "### Antwoord:\n",
    "{example['output']}\"\"\"\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "model_name = \"openchat/openchat-3.5-0106\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# Collator voor causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./openchat-summarizer\",\n",
    "    per_device_train_batch_size=1,  # pas aan naar je GPU\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Opslaan\n",
    "model.save_pretrained(\"./openchat-summarizer\")\n",
    "tokenizer.save_pretrained(\"./openchat-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f5169",
   "metadata": {},
   "source": [
    "# Evaluatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82efed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    \"chat\": [\n",
    "        \"Hoi allemaal! Ze willen windmolens bouwen buiten het dorp. Wat vinden jullie? Er zijn zorgen over geluidsoverlast en impact op het landschap.\",\n",
    "        \"We hebben een bijeenkomst gepland over het nieuwe buurthuis. Mensen willen weten of er genoeg budget is en hoe de planning eruitziet.\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Er is discussie over de bouw van windmolens buiten het dorp, met zorgen over geluid en landschap.\",\n",
    "        \"Er komt een bijeenkomst over het buurthuis, met vragen over budget en planning.\"\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acc806df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumary = \"Buurtbewoners bespreken het plan voor een nieuw parkeerterrein aan de dorpsrand. Arthur en Koen vinden extra parkeerplaatsen positief, maar Fatima maakt zich zorgen over de nabijheid van het speelveldje. Linda vreest voor het uitzicht vanuit haar woning. Peter meldt dat het een groen terrein wordt met bomen, wat positief ontvangen wordt. Koen wijst erop dat het aantal plekken teruggaat van 60 naar 45 om ruimte te maken voor groen. Fatima en Linda stellen voor een groenstrook aan te leggen tussen het speelveld en het parkeerterrein, voor veiligheid en minder zicht. Linda wil dat de gemeente een visualisatie deelt, en Peter stelt voor mee te denken over de inrichting. Allen zijn het erover eens dat bewoners betrokken moeten worden bij de plannen en gezamenlijk suggesties aan de gemeente moeten doen.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6af9cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./bart-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar Jullie meningen! Nou, ik vind het wel een goed ideae, toch?! 🤩 Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden. Gerrit, Jordy, Gerrit and Gerrit are all in onze suggesties. Jordy is in de zomer, Jordi is in the zomer van de verkeersdruk van de straat, Gerhard is in in the Zomer van der Gerrit van der Berrit. Gerret is in het zomer. Jordi, Jordie, Jorden, Gerret, Gerrito, Gerrel, Gerri, Gerrik and Gerrel are in de Zomervan van de Gerrit Van der Berth van der Parkeerrein. Ger\n",
      "\n",
      "Evaluating: ./long-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tetttetttetttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "\n",
      "Evaluating: ./flan-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Precies Koen! Een groenstrook zou om het te vinden voor een nieuw parkeerterrein aan de rand van de dorp? Goed idee or juist niet? Ben benieuwd naar jullie meningen!\n",
      "\n",
      "Evaluating: ./longformer-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullie meningen! Nou, ik vind het parkeerterrein aan de rand van het dorp. Goed idee of juist niet.\n",
      "\n",
      "Evaluating: ./t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Ben benieuwd naar juist niet? Nou, ik vind hehet wel en goed idee Fatima. Spelen is belangrijk.\n",
      "\n",
      "Evaluating: ./pegasus-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\u001b[32m     36\u001b[39m summarizer = pipeline(\u001b[33m\"\u001b[39m\u001b[33msummarization\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer)\n\u001b[32m     38\u001b[39m predictions = [\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m248\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33msummary_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m conversation_text \u001b[38;5;129;01min\u001b[39;00m conversations[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     41\u001b[39m ]\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions[:\u001b[32m5\u001b[39m]:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:280\u001b[39m, in \u001b[36mSummarizationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    257\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    259\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03m          ids of the summary.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:173\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    145\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    147\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    175\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    176\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    177\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    178\u001b[39m     ):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1379\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1386\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1385\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:202\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    200\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2358\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2347\u001b[39m     warnings.warn(\n\u001b[32m   2348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are calling .generate() with the `input_ids` being on a device type different\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m than your model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms device. `input_ids` is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids.device.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, whereas the model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2354\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   2355\u001b[39m     )\n\u001b[32m   2357\u001b[39m \u001b[38;5;66;03m# 9. prepare logits processors and stopping criteria\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2358\u001b[39m prepared_logits_processor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2369\u001b[39m prepared_stopping_criteria = \u001b[38;5;28mself\u001b[39m._get_stopping_criteria(\n\u001b[32m   2370\u001b[39m     generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n\u001b[32m   2371\u001b[39m )\n\u001b[32m   2373\u001b[39m \u001b[38;5;66;03m# Set model_kwargs `use_cache` so we can use it later in forward runs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1130\u001b[39m, in \u001b[36mGenerationMixin._get_logits_processor\u001b[39m\u001b[34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[39m\n\u001b[32m   1123\u001b[39m     processors.append(\n\u001b[32m   1124\u001b[39m         ForcedBOSTokenLogitsProcessor(\n\u001b[32m   1125\u001b[39m             generation_config.forced_bos_token_id,\n\u001b[32m   1126\u001b[39m         )\n\u001b[32m   1127\u001b[39m     )\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.forced_eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     processors.append(\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[43mForcedEOSTokenLogitsProcessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforced_eos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m     )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.remove_invalid_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     processors.append(InfNanRemoveLogitsProcessor())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\generation\\logits_process.py:1595\u001b[39m, in \u001b[36mForcedEOSTokenLogitsProcessor.__init__\u001b[39m\u001b[34m(self, max_length, eos_token_id, device)\u001b[39m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eos_token_id, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   1594\u001b[39m         eos_token_id = [eos_token_id]\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m     eos_token_id = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28mself\u001b[39m.eos_token_id = eos_token_id\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_floating_point(eos_token_id) \u001b[38;5;129;01mor\u001b[39;00m (eos_token_id < \u001b[32m0\u001b[39m).any():\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import evaluate\n",
    "\n",
    "# JSON bestand inladen\n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['summary'] = sumary  # Voeg de samenvatting toe aan de DataFrame\n",
    "# Groeperen per gesprek (topic_id) en alle 'text' regels samenvoegen tot 1 tekst per gesprek\n",
    "conversations = df.groupby(\"topic_id\").agg({\n",
    "    \"text\": lambda texts: \" \".join(texts),  # alle berichten samenvoegen\n",
    "    \"summary\": \"first\"  # neem de summary (die voor alle regels gelijk is)\n",
    "}).reset_index()\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = []\n",
    "\n",
    "model_paths = [\n",
    "    \"./bart-summarizer\",\n",
    "    \"./long-t5-summarizer\",\n",
    "    \"./flan-t5-summarizer\",\n",
    "    \"./longformer-summarizer\",\n",
    "    \"./t5-summarizer\",\n",
    "    \"./pegasus-summarizer\",\n",
    "]\n",
    "\n",
    "for model_path in model_paths:\n",
    "    print(f\"\\nEvaluating: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    predictions = [\n",
    "        summarizer(conversation_text, max_length=248, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "        for conversation_text in conversations[\"text\"]\n",
    "    ]\n",
    "\n",
    "    for pred in predictions[:5]:\n",
    "        print(f\"Prediction: {pred}\")\n",
    "\n",
    "    scores = rouge.compute(predictions=predictions, references=conversations[\"summary\"])\n",
    "    results.append({\n",
    "        \"model\": model_path,\n",
    "        \"rouge1\": round(scores[\"rouge1\"], 4),\n",
    "        \"rouge2\": round(scores[\"rouge2\"], 4),\n",
    "        \"rougeL\": round(scores[\"rougeL\"], 4),\n",
    "        \"rougeLsum\": round(scores[\"rougeLsum\"], 4)\n",
    "    })\n",
    "\n",
    "# Resultaten tonen\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n=== ROUGE Vergelijking ===\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103dd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bb80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./bart-summarizer</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./long-t5-summarizer</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./flan-t5-summarizer</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./longformer-summarizer</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./t5-summarizer</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./pegasus-summarizer</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.1090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  rouge1  rouge2  rougeL  rougeLsum\n",
       "0        ./bart-summarizer  0.4879  0.1417  0.4337     0.4337\n",
       "1     ./long-t5-summarizer  0.0000  0.0000  0.0000     0.0000\n",
       "2     ./flan-t5-summarizer  0.4366  0.1417  0.3824     0.3824\n",
       "3  ./longformer-summarizer  0.4879  0.1417  0.4337     0.4337\n",
       "4          ./t5-summarizer  0.4495  0.1023  0.3824     0.3824\n",
       "5     ./pegasus-summarizer  0.1090  0.0217  0.1090     0.1090"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5479051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullie meningen! Nou, ik vind het wel een goed idee! 🚗 Meer parkeerplekken is nooit verkeerd, toch?! 🤩 Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden voor de kinderen 😕 Daar heb je een goed punt Fatima. Spelen is belangrijk. Maar eerlijk gezegd vind ik het soms echt zoeken naar een parkeerplek als ik thuiskom van werk... 😓 Wat ik me afvraag: wordt het een open terrein of een parkeergarage? Want als het open wordt, dan kijk ik straks tegen auto's aan vanuit mijn keukenraam. 😐 Ik heb gelezen dat het een groen parkeerterrein wordt met bomen ertussen. Dat zou wel schelen qua aanzicht en hitte in de zomer 🌳🌳 Een groen terrein klinkt wel als een mooie middenweg. Maar betekent dat ook minder plekken dan oorspronkelijk gepland? Ja, volgens de gemeentepagina worden het 45 plekken i.p.v. 60. Minder, maar wel groener en beter ingepast. 🌱 Zolang het speelveld blijft en er geen extra verkeersdruk komt in de straat, kan ik er wel mee leven. Misschien moeten we de gemeente vragen of ze een maquette of visualisatie willen delen. Dan weten we tenminste waar we aan toe zijn. Goed idee Linda. En misschien kunnen we ook meedenken over de inrichting? Bankjes? Groene hagen? 🌼 Laten we dat zeker voorstellen. Als we samenkomen met wat suggesties, staat de gemeente daar vaak wel voor open. 💬 Ik vind het ook belangrijk dat we als buurt betrokken worden bij dit soort plannen. Het gaat tenslotte om onze leefomgeving! 🏡 Precies Koen! En misschien kunnen we ook vragen of er een groenstrook komt tussen het parkeerterrein en het speelveldje? Dat zou het voor de kinderen veiliger maken. Dat is een goed idee Fatima! Een groenstrook zou ook helpen om het zicht te beperken. Laten we dat zeker meenemen in onze suggesties. Ik ben blij dat we dit bespreken. Het is belangrijk dat we als buurt samen optrekken en onze stem laten horen! 💪\n",
      "📝 Samenvatting van de discussie:\n",
      "Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar Jullie meningen! Nou, ik vind het wel een goed ideen, toch?! 🤩 Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden Voor de\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Laad model\n",
    "model_path = \"./bart-summarizer\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# JSON string met één bericht\n",
    "\n",
    "# Laad het JSON-bestand \n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Combineer alle tekstberichten\n",
    "chat_text = \" \".join([msg[\"text\"] for msg in data])\n",
    "print(chat_text)\n",
    "\n",
    "# Genereer samenvatting\n",
    "summary = summarizer(chat_text, max_length=128, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "\n",
    "# Toon output\n",
    "print(\"📝 Samenvatting van de discussie:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d6729",
   "metadata": {},
   "source": [
    "# COde voor mark. Hier gebruiken we onze gefinetuned model van bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcd479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give mainly the opinions of the people: “Linda”, “Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar Jullie meningen!” “Nou, ik vind het wel een goed Idee, toch?! 🤩”“Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden van de kinderen 😕“ “I’m a man who’s a woman who is a woman in her 20s.” 💪“We don’t know if we’re going to get to know her, but we do know that she’ll be a woman soon.“She is a man in her 30s!““What if she is\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "def preprocces_json(json_file):\n",
    "    \"\"\"\n",
    "    Laad een JSON-bestand en retourneer de tekstberichten.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "    chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "    input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "    return input_text\n",
    "\n",
    "def voormark(input_text, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    summary = summarizer(\n",
    "        input_text,\n",
    "        max_length=248,\n",
    "        min_length=50,\n",
    "        do_sample=True,\n",
    "        num_beams=4\n",
    "    )[0][\"summary_text\"]\n",
    "    return summary\n",
    "input_text = preprocces_json(\"eval/eval1.json\")\n",
    "summary = voormark(input_text, \"./bart-summarizer\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed54b140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give mainly the opinions of the people: 🌱Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullie meningen! 🌱Hoi allemaal! 🌱Hoi allemaal! 🌱Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Ben benieuwd naar jullie meningen! 🌱Hoi allemaal! 🌱Hoi allemaal! 🌱Hoi allemaal! 🌱Hoi allemaal! 🌱Hoi snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden voor de kinderen 😕Hoi allemaal! �\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"allenai/led-base-16384\")\n",
    "\n",
    "# Laad het JSON-bestand\n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "# Voeg een instructie toe voor betere focus\n",
    "input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "\n",
    "# Samenvatten met beam search en iets langere minimumlengte\n",
    "summary = summarizer(\n",
    "    input_text,\n",
    "    max_length=248,\n",
    "    min_length=50,\n",
    "    do_sample=True,\n",
    "    num_beams=4\n",
    ")[0][\"summary_text\"]\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b9c6a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give mainly the opinions of the people: ~~Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullies meningen! ~~Nou, ik vind het wel een goed idea! 🚗 Meer parkeersplekken is nooit verkeerd, toch?! 🤩 ~~Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden! 💪 ~~Fatima’s keușen is belangrijk. Maar eerlijk gezegd vind ik het soms echt zoeken naar een Parkeerplek als ik thuiskom van werk... 😕@@@@@@@@Daar heb je echt samenkomen met wat\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")\n",
    "\n",
    "# Laad het JSON-bestand\n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "# Voeg een instructie toe voor betere focus\n",
    "input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "\n",
    "# Samenvatten met beam search en iets langere minimumlengte\n",
    "summary = summarizer(\n",
    "    input_text,\n",
    "    max_length=248,\n",
    "    min_length=50,\n",
    "    do_sample=True,\n",
    "    num_beams=4\n",
    ")[0][\"summary_text\"]\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb012304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A parkeerterrein aan de rand van het dorp is planned. The people of the dorp had a conversation about the plannen. They were asked what they thought of the idea of a nieuw parkeersdruk. They said it was a goed idee of juist niet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Laad het JSON-bestand\n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "# Voeg een instructie toe voor betere focus\n",
    "input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "\n",
    "# Samenvatten met beam search en iets langere minimumlengte\n",
    "summary = summarizer(\n",
    "    input_text,\n",
    "    max_length=248,\n",
    "    min_length=50,\n",
    "    do_sample=True,\n",
    "    num_beams=4\n",
    ")[0][\"summary_text\"]\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc6083b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
