{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b35c00",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In deze notebook gaan we verschillende pre-trained modellen finetunen op onze gegenereerde data. We hebben deze modellen gekozen op verschillende eisen. Waar zijn ze op getrained? hHoe groot zijn de modellen? Zijn ze opensource? enz.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10872 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_002_20250605_110618.json\n",
      "165 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_003_20250605_111208.json\n",
      "170 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_004_20250605_111802.json\n",
      "166 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_005_20250605_112316.json\n",
      "172 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_006_20250605_112912.json\n",
      "161 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_007_20250605_113423.json\n",
      "164 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_008_20250605_114010.json\n",
      "165 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_009_20250605_114525.json\n",
      "173 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_010_20250605_121148.json\n",
      "✅ Dataset opgeslagen als samengevoegd_met_samenvattingen_local.jsonl met 9 items.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_PATH = r\"C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\"\n",
    "OUTPUT_FILE = \"samengevoegd_met_samenvattingen_local.jsonl\"\n",
    "MODEL_NAME = \"philschmid/bart-large-cnn-samsum\"\n",
    "\n",
    "# === SAMENVATTINGSMODEL EN TOKENIZER LADEN ===\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=MODEL_NAME, tokenizer=MODEL_NAME, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_INPUT_LENGTH = tokenizer.model_max_length\n",
    "\n",
    "def split_text_by_tokens(text, tokenizer, max_input_tokens):\n",
    "    \"\"\"\n",
    "    Splitst een lange tekst in stukken die elk maximaal max_input_tokens tokens bevatten.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_input_tokens):\n",
    "        chunk_tokens = tokens[i:i+max_input_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# === BESTANDEN VERWERKEN ===\n",
    "all_chat_data = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(DATA_PATH, \"*.json\")):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        print(len(data), \"items gevonden in\", filepath)\n",
    "    topics = defaultdict(list)\n",
    "    for item in data:\n",
    "        topics[item[\"topic_id\"]].append(item[\"text\"])\n",
    "\n",
    "    for topic_id, messages in topics.items():\n",
    "        combined_text = \" \".join(messages)\n",
    "        if len(combined_text.strip()) < 50:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text_chunks = split_text_by_tokens(combined_text, tokenizer, MAX_INPUT_LENGTH - 5)\n",
    "            summaries = []\n",
    "\n",
    "            for chunk in text_chunks:\n",
    "                if not chunk.strip():\n",
    "                    continue\n",
    "                summary_output = summarizer(\n",
    "                    chunk,\n",
    "                    max_length=100,\n",
    "                    min_length=30,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "                summaries.append(summary_output[0][\"summary_text\"])\n",
    "\n",
    "            full_summary = \" \".join(summaries)\n",
    "            if not full_summary.strip():\n",
    "                full_summary = \"No summary generated.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing topic_id {topic_id}: {e}\")\n",
    "            full_summary = \"Summary not available due to error.\"\n",
    "\n",
    "        all_chat_data.append({\n",
    "            \"topic_id\": topic_id,\n",
    "            \"chat\": combined_text,\n",
    "            \"summary\": full_summary\n",
    "        })\n",
    "\n",
    "# === OPSLAAN ALS JSONL ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in all_chat_data:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Dataset opgeslagen als {OUTPUT_FILE} met {len(all_chat_data)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a967ffc",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze code:\n",
    "- Laadt een BART-samenvattingsmodel (`bart-large-cnn-samsum`) en bijbehorende tokenizer.\n",
    "- Verwerkt JSON-chatdata per `topic_id`, combineert tekst tot één document per onderwerp.\n",
    "- Splits lange documenten op in chunks die passen in het model (minder dan `MAX_INPUT_LENGTH` tokens).\n",
    "- Genereert per chunk een samenvatting en combineert deze.\n",
    "- Slaat alles op als `.jsonl`-bestand waarin elke regel een samenvatting bevat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ab5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map:   0%|          | 0/9 [00:00<?, ? examples/s]c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 14.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Laad je jsonl-bestand in als een lijst van dicts\n",
    "with open(\"samengevoegd_met_samenvattingen_local.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chat_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Maak een Huggingface Dataset\n",
    "dataset = Dataset.from_list(chat_data)\n",
    "# Tokenizer en model laden\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"chat\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd5a54",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Laadt het JSONL-bestand met gegenereerde samenvattingen.\n",
    "- Zet het om naar een Hugging Face `Dataset`.\n",
    "- Laadt `facebook/bart-base` model/tokenizer.\n",
    "- Tokeniseert de 'chat' als input en de 'summary' als target.\n",
    "- Gebruikt padding en truncation om input/output op vaste lengte te brengen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85817f29",
   "metadata": {},
   "source": [
    "## Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f61444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 15.37 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_28992\\3881543212.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart-summarizer\\\\tokenizer_config.json',\n",
       " './bart-summarizer\\\\special_tokens_map.json',\n",
       " './bart-summarizer\\\\vocab.json',\n",
       " './bart-summarizer\\\\merges.txt',\n",
       " './bart-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer en model laden\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"chat\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./bart-summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669439e9",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./bart-summarizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76adb7",
   "metadata": {},
   "source": [
    "## T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ff31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 39.79 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_16024\\1563979243.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.419400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./t5-summarizer\\\\tokenizer_config.json',\n",
       " './t5-summarizer\\\\special_tokens_map.json',\n",
       " './t5-summarizer\\\\spiece.model',\n",
       " './t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77dc5f",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./t5-summarizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63a0e6",
   "metadata": {},
   "source": [
    "## T5-Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-tglobal-base and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 29.91 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_11024\\2989404296.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:06, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>26.186200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./long-t5-summarizer\\\\tokenizer_config.json',\n",
       " './long-t5-summarizer\\\\special_tokens_map.json',\n",
       " './long-t5-summarizer\\\\spiece.model',\n",
       " './long-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./long-t5-summarizer\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,    \n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./long-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./long-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3234170",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./long-t5-summarizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7aaf0",
   "metadata": {},
   "source": [
    "## T5-Flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b28ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 37.66 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_34044\\4066598123.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./flan-t5-summarizer\\\\tokenizer_config.json',\n",
       " './flan-t5-summarizer\\\\special_tokens_map.json',\n",
       " './flan-t5-summarizer\\\\spiece.model',\n",
       " './flan-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./flan-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf55f77",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./flan-t5-summarizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796db99a",
   "metadata": {},
   "source": [
    "## PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b220004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 9/9 [00:00<00:00, 38.07 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_9712\\1184256781.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 08:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.573100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./pegasus-summarizer\\\\tokenizer_config.json',\n",
       " './pegasus-summarizer\\\\special_tokens_map.json',\n",
       " './pegasus-summarizer\\\\spiece.model',\n",
       " './pegasus-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pegasus-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./pegasus-summarizer\")\n",
    "tokenizer.save_pretrained(\"./pegasus-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ecd4c",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset (indien nog niet gebeurd).\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./pegasus-summarizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f83064",
   "metadata": {},
   "source": [
    "## LED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9/9 [00:00<00:00, 32.72 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_9712\\3891717521.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Input ids are automatically padded from 512 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.553600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./longformer-summarizer\\\\tokenizer_config.json',\n",
       " './longformer-summarizer\\\\special_tokens_map.json',\n",
       " './longformer-summarizer\\\\vocab.json',\n",
       " './longformer-summarizer\\\\merges.txt',\n",
       " './longformer-summarizer\\\\added_tokens.json',\n",
       " './longformer-summarizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./longformer-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./longformer-summarizer\")\n",
    "tokenizer.save_pretrained(\"./longformer-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0214cbd",
   "metadata": {},
   "source": [
    "\n",
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset (indien nog niet gebeurd).\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./longformer-summarizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f5169",
   "metadata": {},
   "source": [
    "# Evaluatie\n",
    "\n",
    "In dit deel van de notebook gaan we kijken welke van onze modellen het best presteerd op een kleine evaluatie dataset.\n",
    "\n",
    "Eerst bereken we de ROUGE scoren op een heel simpel dataset. Daarna kijken we zelf met een iets ingewikkeldere chat welke samenvatting van de best scorende model wij beter vinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82efed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    \"chat\": [\n",
    "        \"Hoi allemaal! Ze willen windmolens bouwen buiten het dorp. Wat vinden jullie? Er zijn zorgen over geluidsoverlast en impact op het landschap.\",\n",
    "        \"We hebben een bijeenkomst gepland over het nieuwe buurthuis. Mensen willen weten of er genoeg budget is en hoe de planning eruitziet.\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Er is discussie over de bouw van windmolens buiten het dorp, met zorgen over geluid en landschap.\",\n",
    "        \"Er komt een bijeenkomst over het buurthuis, met vragen over budget en planning.\"\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc806df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumary = \"Buurtbewoners bespreken het plan voor een nieuw parkeerterrein aan de dorpsrand. Arthur en Koen vinden extra parkeerplaatsen positief, maar Fatima maakt zich zorgen over de nabijheid van het speelveldje. Linda vreest voor het uitzicht vanuit haar woning. Peter meldt dat het een groen terrein wordt met bomen, wat positief ontvangen wordt. Koen wijst erop dat het aantal plekken teruggaat van 60 naar 45 om ruimte te maken voor groen. Fatima en Linda stellen voor een groenstrook aan te leggen tussen het speelveld en het parkeerterrein, voor veiligheid en minder zicht. Linda wil dat de gemeente een visualisatie deelt, en Peter stelt voor mee te denken over de inrichting. Allen zijn het erover eens dat bewoners betrokken moeten worden bij de plannen en gezamenlijk suggesties aan de gemeente moeten doen.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af9cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./bart-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 248, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Your max_length is set to 248, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Hoi allemaal! Ze willen windmolens bouwen buiten het dorp. Wat vinden jullie? Er zijn zorgen over geluidsoverlast en impact op het landschap.\n",
      "Prediction: We hebben een bijeenkomst gepland over het nieuwe buurthuis. Mensen willen weten of er genoeg budget is en hoe de planning eruitziet.\n",
      "\n",
      "Evaluating: ./long-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Device set to use cuda:0\n",
      "Your max_length is set to 248, but your input_length is only 66. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Your max_length is set to 248, but your input_length is only 62. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "Prediction: tettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettettet\n",
      "\n",
      "Evaluating: ./flan-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 248, but your input_length is only 68. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Your max_length is set to 248, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Hoi allemaal! U wilen windmolens bouwen buiten het dorp. What do you find jullie? Er zorgen over geluidsoverlast et impact om landskap.\n",
      "Prediction: We hebben een bijeenkomst gepland over het nieuwe buurthuis, mensen willen weten of er genoeg budget is en hoe de planning eruenziet.\n",
      "\n",
      "Evaluating: ./longformer-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 248, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n",
      "Input ids are automatically padded from 52 to 1024 to be a multiple of `config.attention_window`: 1024\n",
      "Your max_length is set to 248, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
      "Input ids are automatically padded from 51 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Hoi allemaal! Ze willen windmolens bouwen buiten het dorp. Wat vinden jullie? Er zijn zorgen over geluidsoverlast en impact op het landschap.\n",
      "Prediction: We hebben een bijeenkomst gepland over het nieuwe buurthuis. Mensen willen weten of er genoeg budget is en hoe de planning is\n",
      "\n",
      "Evaluating: ./t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 248, but your input_length is only 68. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "Your max_length is set to 248, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: zijn zorgen over geluidsoverlast en impact op het landschap.\n",
      "Prediction: we hebben een bijeenkomst gepland over het nieuwe buurthuis . mensen willen weten of er genoeg budget is en hoe de planning eruitziet.\n",
      "\n",
      "Evaluating: ./pegasus-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 248, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "Your max_length is set to 248, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is het dorp, het dorp is he\n",
      "Prediction: Het nieuwe buurthuis is in de eerste eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in de eerste dag, in\n",
      "\n",
      "=== ROUGE Vergelijking ===\n",
      "                  model  rouge1  rouge2  rougeL  rougeLsum\n",
      "      ./bart-summarizer  0.4879  0.1417  0.4337     0.4337\n",
      "   ./long-t5-summarizer  0.0000  0.0000  0.0000     0.0000\n",
      "   ./flan-t5-summarizer  0.4366  0.1417  0.3824     0.3824\n",
      "./longformer-summarizer  0.4879  0.1417  0.4337     0.4337\n",
      "        ./t5-summarizer  0.4495  0.1023  0.3824     0.3824\n",
      "   ./pegasus-summarizer  0.0361  0.0072  0.0361     0.0361\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import evaluate\n",
    "\n",
    "conversations = test_data\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = []\n",
    "\n",
    "model_paths = [\n",
    "    \"./bart-summarizer\",\n",
    "    \"./long-t5-summarizer\",\n",
    "    \"./flan-t5-summarizer\",\n",
    "    \"./longformer-summarizer\",\n",
    "    \"./t5-summarizer\",\n",
    "    \"./pegasus-summarizer\",\n",
    "]\n",
    "\n",
    "for model_path in model_paths:\n",
    "    print(f\"\\nEvaluating: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    predictions = [\n",
    "        summarizer(conversation_text, max_length=248, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "        for conversation_text in conversations[\"chat\"]\n",
    "    ]\n",
    "\n",
    "    for pred in predictions[:5]:\n",
    "        print(f\"Prediction: {pred}\")\n",
    "\n",
    "    scores = rouge.compute(predictions=predictions, references=conversations[\"summary\"])\n",
    "    results.append({\n",
    "        \"model\": model_path,\n",
    "        \"rouge1\": round(scores[\"rouge1\"], 4),\n",
    "        \"rouge2\": round(scores[\"rouge2\"], 4),\n",
    "        \"rougeL\": round(scores[\"rougeL\"], 4),\n",
    "        \"rougeLsum\": round(scores[\"rougeLsum\"], 4)\n",
    "    })\n",
    "\n",
    "# Resultaten tonen\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n=== ROUGE Vergelijking ===\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7fdd3",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze code:\n",
    "- Laadt een BART-samenvattingsmodel (`bart-large-cnn-samsum`) en bijbehorende tokenizer.\n",
    "- Verwerkt JSON-chatdata per `topic_id`, combineert tekst tot één document per onderwerp.\n",
    "- Splits lange documenten op in chunks die passen in het model (minder dan `MAX_INPUT_LENGTH` tokens).\n",
    "- Genereert per chunk een samenvatting en combineert deze.\n",
    "- Slaat alles op als `.jsonl`-bestand waarin elke regel een samenvatting bevat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bb80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./bart-summarizer</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./long-t5-summarizer</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./flan-t5-summarizer</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./longformer-summarizer</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./t5-summarizer</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./pegasus-summarizer</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.0361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  rouge1  rouge2  rougeL  rougeLsum\n",
       "0        ./bart-summarizer  0.4879  0.1417  0.4337     0.4337\n",
       "1     ./long-t5-summarizer  0.0000  0.0000  0.0000     0.0000\n",
       "2     ./flan-t5-summarizer  0.4366  0.1417  0.3824     0.3824\n",
       "3  ./longformer-summarizer  0.4879  0.1417  0.4337     0.4337\n",
       "4          ./t5-summarizer  0.4495  0.1023  0.3824     0.3824\n",
       "5     ./pegasus-summarizer  0.0361  0.0072  0.0361     0.0361"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a210a98",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Hier kan je zien in de dataframe dat Bart en Longformer het beste presteren. Maar als we gaan kijken naar hoe de samenvatting er echt uit ziet. Vinden wij zelf dat die van bart er het best uit ziet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660aff65",
   "metadata": {},
   "source": [
    "Hier kijken we eerst hoe bart zonder finetunen de samenvatting zou genereren. We kunnen zien dat hij er erg veel moeite mee heeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b9c6a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give mainly the opinions of the people: ~~Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullies meningen! ~~Nou, ik vind het wel een goed idea! 🚗 Meer parkeersplekken is nooit verkeerd, toch?! 🤩 ~~Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden! 💪 ~~Fatima’s keușen is belangrijk. Maar eerlijk gezegd vind ik het soms echt zoeken naar een Parkeerplek als ik thuiskom van werk... 😕@@@@@@@@Daar heb je echt samenkomen met wat\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")\n",
    "\n",
    "# Laad het JSON-bestand\n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "# Voeg een instructie toe voor betere focus\n",
    "input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "\n",
    "# Samenvatten met beam search en iets langere minimumlengte\n",
    "summary = summarizer(\n",
    "    input_text,\n",
    "    max_length=248,\n",
    "    min_length=50,\n",
    "    do_sample=True,\n",
    "    num_beams=4\n",
    ")[0][\"summary_text\"]\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d6729",
   "metadata": {},
   "source": [
    "# COde voor mark. Hier gebruiken we onze gefinetuned model van bart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1d48e",
   "metadata": {},
   "source": [
    "Hier vegelijken we de outputs van Longformer en Bart. Zoals je kan zien zijn het niet de beste samenvattingen. Dat komt omdat onze data niet van goede kwaliteit is. Ook hebben we niet zo veel data. En de modellen van zich zelf presteren voor het finetunen al erg matig met het samenvatten van onze chat berichten. Als we de outputs vergelijken van bart voor het finetunen en na het finetunen kunnen we wel zien dat het model iets heeft geleerd. Zo is het model na het finetunen toch wel iets beter in een samenvatting genereren dan het pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfdf860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give me your thoughts about the parkeerterrein in de rand van het dorp.Jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullie meningen!Nou, ik vind het wel een goed idee! 🚗 Meer parkeerterrein aan de rand van het dorp\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "def preprocces_json(json_file):\n",
    "    \"\"\"\n",
    "    Laad een JSON-bestand en retourneer de tekstberichten.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "    chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "    input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "    return input_text\n",
    "\n",
    "def voormark(input_text, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    summary = summarizer(\n",
    "        input_text,\n",
    "        max_length=248,\n",
    "        min_length=50,\n",
    "        do_sample=True,\n",
    "        num_beams=4\n",
    "    )[0][\"summary_text\"]\n",
    "    return summary\n",
    "input_text = preprocces_json(\"eval/eval1.json\")\n",
    "summary = voormark(input_text, \"./longformer-summarizer\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcd479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give mainly the opinions of the people: “Linda”, “Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar Jullie meningen!” “Nou, ik vind het wel een goed Idee, toch?! 🤩”“Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden van de kinderen 😕“ “I’m a man who’s a woman who is a woman in her 20s.” 💪“We don’t know if we’re going to get to know her, but we do know that she’ll be a woman soon.“She is a man in her 30s!““What if she is\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "def preprocces_json(json_file):\n",
    "    \"\"\"\n",
    "    Laad een JSON-bestand en retourneer de tekstberichten.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "    chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "    input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "    return input_text\n",
    "\n",
    "def voormark(input_text, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    summary = summarizer(\n",
    "        input_text,\n",
    "        max_length=248,\n",
    "        min_length=50,\n",
    "        do_sample=True,\n",
    "        num_beams=4\n",
    "    )[0][\"summary_text\"]\n",
    "    return summary\n",
    "input_text = preprocces_json(\"eval/eval1.json\")\n",
    "summary = voormark(input_text, \"./bart-summarizer\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
