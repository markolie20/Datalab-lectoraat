{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b35c00",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "In deze notebook gaan we verschillende pre-trained modellen finetunen op onze gegenereerde data. We hebben deze modellen gekozen op verschillende eisen. Waar zijn ze op getrained? hHoe groot zijn de modellen? Zijn ze opensource? enz.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10872 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_002_20250605_110618.json\n",
      "165 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_003_20250605_111208.json\n",
      "170 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_004_20250605_111802.json\n",
      "166 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_005_20250605_112316.json\n",
      "172 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_006_20250605_112912.json\n",
      "161 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_007_20250605_113423.json\n",
      "164 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_008_20250605_114010.json\n",
      "165 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_009_20250605_114525.json\n",
      "173 items gevonden in C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\\chatlog_topic_010_20250605_121148.json\n",
      "âœ… Dataset opgeslagen als samengevoegd_met_samenvattingen_local.jsonl met 9 items.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "DATA_PATH = r\"C:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\Testing\\data\"\n",
    "OUTPUT_FILE = \"samengevoegd_met_samenvattingen_local.jsonl\"\n",
    "MODEL_NAME = \"philschmid/bart-large-cnn-samsum\"\n",
    "\n",
    "# === SAMENVATTINGSMODEL EN TOKENIZER LADEN ===\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use: {'cuda' if device == 0 else 'cpu'}\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=MODEL_NAME, tokenizer=MODEL_NAME, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_INPUT_LENGTH = tokenizer.model_max_length\n",
    "\n",
    "def split_text_by_tokens(text, tokenizer, max_input_tokens):\n",
    "    \"\"\"\n",
    "    Splitst een lange tekst in stukken die elk maximaal max_input_tokens tokens bevatten.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_input_tokens):\n",
    "        chunk_tokens = tokens[i:i+max_input_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    return chunks\n",
    "\n",
    "# === BESTANDEN VERWERKEN ===\n",
    "all_chat_data = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(DATA_PATH, \"*.json\")):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        print(len(data), \"items gevonden in\", filepath)\n",
    "    topics = defaultdict(list)\n",
    "    for item in data:\n",
    "        topics[item[\"topic_id\"]].append(item[\"text\"])\n",
    "\n",
    "    for topic_id, messages in topics.items():\n",
    "        combined_text = \" \".join(messages)\n",
    "        if len(combined_text.strip()) < 50:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            text_chunks = split_text_by_tokens(combined_text, tokenizer, MAX_INPUT_LENGTH - 5)\n",
    "            summaries = []\n",
    "\n",
    "            for chunk in text_chunks:\n",
    "                if not chunk.strip():\n",
    "                    continue\n",
    "                summary_output = summarizer(\n",
    "                    chunk,\n",
    "                    max_length=100,\n",
    "                    min_length=30,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "                summaries.append(summary_output[0][\"summary_text\"])\n",
    "\n",
    "            full_summary = \" \".join(summaries)\n",
    "            if not full_summary.strip():\n",
    "                full_summary = \"No summary generated.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing topic_id {topic_id}: {e}\")\n",
    "            full_summary = \"Summary not available due to error.\"\n",
    "\n",
    "        all_chat_data.append({\n",
    "            \"topic_id\": topic_id,\n",
    "            \"chat\": combined_text,\n",
    "            \"summary\": full_summary\n",
    "        })\n",
    "\n",
    "# === OPSLAAN ALS JSONL ===\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in all_chat_data:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Dataset opgeslagen als {OUTPUT_FILE} met {len(all_chat_data)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a967ffc",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze code:\n",
    "- Laadt een BART-samenvattingsmodel (`bart-large-cnn-samsum`) en bijbehorende tokenizer.\n",
    "- Verwerkt JSON-chatdata per `topic_id`, combineert tekst tot Ã©Ã©n document per onderwerp.\n",
    "- Splits lange documenten op in chunks die passen in het model (minder dan `MAX_INPUT_LENGTH` tokens).\n",
    "- Genereert per chunk een samenvatting en combineert deze.\n",
    "- Slaat alles op als `.jsonl`-bestand waarin elke regel een samenvatting bevat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ab5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map:   0%|          | 0/9 [00:00<?, ? examples/s]c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 14.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Laad je jsonl-bestand in als een lijst van dicts\n",
    "with open(\"samengevoegd_met_samenvattingen_local.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chat_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Maak een Huggingface Dataset\n",
    "dataset = Dataset.from_list(chat_data)\n",
    "# Tokenizer en model laden\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"chat\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd5a54",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Laadt het JSONL-bestand met gegenereerde samenvattingen.\n",
    "- Zet het om naar een Hugging Face `Dataset`.\n",
    "- Laadt `facebook/bart-base` model/tokenizer.\n",
    "- Tokeniseert de 'chat' als input en de 'summary' als target.\n",
    "- Gebruikt padding en truncation om input/output op vaste lengte te brengen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85817f29",
   "metadata": {},
   "source": [
    "## Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f61444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 15.37 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_28992\\3881543212.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bart-summarizer\\\\tokenizer_config.json',\n",
       " './bart-summarizer\\\\special_tokens_map.json',\n",
       " './bart-summarizer\\\\vocab.json',\n",
       " './bart-summarizer\\\\merges.txt',\n",
       " './bart-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Tokenizer en model laden\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    inputs = tokenizer(example[\"chat\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=False)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bart-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./bart-summarizer\")\n",
    "tokenizer.save_pretrained(\"./bart-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669439e9",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./bart-summarizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76adb7",
   "metadata": {},
   "source": [
    "## T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ff31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 39.79 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_16024\\1563979243.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.419400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./t5-summarizer\\\\tokenizer_config.json',\n",
       " './t5-summarizer\\\\special_tokens_map.json',\n",
       " './t5-summarizer\\\\spiece.model',\n",
       " './t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb77dc5f",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./t5-summarizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a63a0e6",
   "metadata": {},
   "source": [
    "## T5-Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f4acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/long-t5-tglobal-base and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 29.91 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_11024\\2989404296.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:06, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>26.186200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./long-t5-summarizer\\\\tokenizer_config.json',\n",
       " './long-t5-summarizer\\\\special_tokens_map.json',\n",
       " './long-t5-summarizer\\\\spiece.model',\n",
       " './long-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/long-t5-tglobal-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./long-t5-summarizer\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,    \n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./long-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./long-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3234170",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./long-t5-summarizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7aaf0",
   "metadata": {},
   "source": [
    "## T5-Flan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b28ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 37.66 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_34044\\4066598123.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./flan-t5-summarizer\\\\tokenizer_config.json',\n",
       " './flan-t5-summarizer\\\\special_tokens_map.json',\n",
       " './flan-t5-summarizer\\\\spiece.model',\n",
       " './flan-t5-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./flan-t5-summarizer\")\n",
    "tokenizer.save_pretrained(\"./flan-t5-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf55f77",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset.\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./flan-t5-summarizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796db99a",
   "metadata": {},
   "source": [
    "## PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b220004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 38.07 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_9712\\1184256781.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 08:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.573100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'num_beams': 8, 'length_penalty': 0.6}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./pegasus-summarizer\\\\tokenizer_config.json',\n",
       " './pegasus-summarizer\\\\special_tokens_map.json',\n",
       " './pegasus-summarizer\\\\spiece.model',\n",
       " './pegasus-summarizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pegasus-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./pegasus-summarizer\")\n",
    "tokenizer.save_pretrained(\"./pegasus-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ecd4c",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset (indien nog niet gebeurd).\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./pegasus-summarizer`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f83064",
   "metadata": {},
   "source": [
    "## LED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 32.72 examples/s]\n",
      "C:\\Users\\caspe\\AppData\\Local\\Temp\\ipykernel_9712\\3891717521.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Input ids are automatically padded from 512 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.553600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./longformer-summarizer\\\\tokenizer_config.json',\n",
       " './longformer-summarizer\\\\special_tokens_map.json',\n",
       " './longformer-summarizer\\\\vocab.json',\n",
       " './longformer-summarizer\\\\merges.txt',\n",
       " './longformer-summarizer\\\\added_tokens.json',\n",
       " './longformer-summarizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Model en tokenizer laden\n",
    "model_name = \"allenai/led-base-16384\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing-functie\n",
    "def preprocess_function(example):\n",
    "    input_text = \"summarize: \" + example[\"chat\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels = tokenizer(example[\"summary\"], max_length=64, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./longformer-summarizer\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    "    fp16=True,\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer opzetten\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Model opslaan\n",
    "model.save_pretrained(\"./longformer-summarizer\")\n",
    "tokenizer.save_pretrained(\"./longformer-summarizer\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0214cbd",
   "metadata": {},
   "source": [
    "\n",
    "**Uitleg:**  \n",
    "Deze cel:\n",
    "- Herhaalt het tokeniseren van de dataset (indien nog niet gebeurd).\n",
    "- Stelt de `TrainingArguments` in, o.a.:\n",
    "  - `batch_size = 4`\n",
    "  - `epochs = 1`\n",
    "  - `learning_rate = 2e-5`\n",
    "- Maakt gebruik van `Trainer` om het model te trainen.\n",
    "- Slaat het getrainde model en tokenizer op in `./longformer-summarizer`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f5169",
   "metadata": {},
   "source": [
    "# Evaluatie\n",
    "\n",
    "In dit deel van de notebook gaan we kijken welke van onze modellen het best presteerd op een kleine evaluatie dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82efed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_data = Dataset.from_dict({\n",
    "    \"chat\": [\n",
    "        \"Hoi allemaal! Ze willen windmolens bouwen buiten het dorp. Wat vinden jullie? Er zijn zorgen over geluidsoverlast en impact op het landschap.\",\n",
    "        \"We hebben een bijeenkomst gepland over het nieuwe buurthuis. Mensen willen weten of er genoeg budget is en hoe de planning eruitziet.\"\n",
    "    ],\n",
    "    \"summary\": [\n",
    "        \"Er is discussie over de bouw van windmolens buiten het dorp, met zorgen over geluid en landschap.\",\n",
    "        \"Er komt een bijeenkomst over het buurthuis, met vragen over budget en planning.\"\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acc806df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumary = \"Buurtbewoners bespreken het plan voor een nieuw parkeerterrein aan de dorpsrand. Arthur en Koen vinden extra parkeerplaatsen positief, maar Fatima maakt zich zorgen over de nabijheid van het speelveldje. Linda vreest voor het uitzicht vanuit haar woning. Peter meldt dat het een groen terrein wordt met bomen, wat positief ontvangen wordt. Koen wijst erop dat het aantal plekken teruggaat van 60 naar 45 om ruimte te maken voor groen. Fatima en Linda stellen voor een groenstrook aan te leggen tussen het speelveld en het parkeerterrein, voor veiligheid en minder zicht. Linda wil dat de gemeente een visualisatie deelt, en Peter stelt voor mee te denken over de inrichting. Allen zijn het erover eens dat bewoners betrokken moeten worden bij de plannen en gezamenlijk suggesties aan de gemeente moeten doen.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6af9cdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: ./bart-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar Jullie meningen! Nou, ik vind het wel een goed ideae, toch?! ðŸ¤© Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden. Gerrit, Jordy, Gerrit and Gerrit are all in onze suggesties. Jordy is in de zomer, Jordi is in the zomer van de verkeersdruk van de straat, Gerhard is in in the Zomer van der Gerrit van der Berrit. Gerret is in het zomer. Jordi, Jordie, Jorden, Gerret, Gerrito, Gerrel, Gerri, Gerrik and Gerrel are in de Zomervan van de Gerrit Van der Berth van der Parkeerrein. Ger\n",
      "\n",
      "Evaluating: ./long-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tetttetttetttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "\n",
      "Evaluating: ./flan-t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Precies Koen! Een groenstrook zou om het te vinden voor een nieuw parkeerterrein aan de rand van de dorp? Goed idee or juist niet? Ben benieuwd naar jullie meningen!\n",
      "\n",
      "Evaluating: ./longformer-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Input ids are automatically padded from 794 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullie meningen! Nou, ik vind het parkeerterrein aan de rand van het dorp. Goed idee of juist niet.\n",
      "\n",
      "Evaluating: ./t5-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (954 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Ben benieuwd naar juist niet? Nou, ik vind hehet wel en goed idee Fatima. Spelen is belangrijk.\n",
      "\n",
      "Evaluating: ./pegasus-summarizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (749 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     34\u001b[39m model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\u001b[32m     36\u001b[39m summarizer = pipeline(\u001b[33m\"\u001b[39m\u001b[33msummarization\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer)\n\u001b[32m     38\u001b[39m predictions = [\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversation_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m248\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33msummary_text\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m conversation_text \u001b[38;5;129;01min\u001b[39;00m conversations[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     41\u001b[39m ]\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions[:\u001b[32m5\u001b[39m]:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:280\u001b[39m, in \u001b[36mSummarizationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    257\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m    Summarize the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    259\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03m          ids of the summary.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:173\u001b[39m, in \u001b[36mText2TextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    145\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[32m    147\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m \u001b[33;03m          ids of the generated text.\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    175\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    176\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[32m0\u001b[39m])\n\u001b[32m    177\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) == \u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[32m    178\u001b[39m     ):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1379\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1371\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1372\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1373\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1376\u001b[39m         )\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1386\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1385\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1285\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1286\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1287\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1288\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py:202\u001b[39m, in \u001b[36mText2TextGenerationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs, **generate_kwargs)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[32m    200\u001b[39m     generate_kwargs[\u001b[33m\"\u001b[39m\u001b[33mgeneration_config\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.generation_config\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m out_b = output_ids.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2358\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2347\u001b[39m     warnings.warn(\n\u001b[32m   2348\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are calling .generate() with the `input_ids` being on a device type different\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2349\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m than your model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms device. `input_ids` is on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids.device.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, whereas the model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2354\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   2355\u001b[39m     )\n\u001b[32m   2357\u001b[39m \u001b[38;5;66;03m# 9. prepare logits processors and stopping criteria\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2358\u001b[39m prepared_logits_processor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2368\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2369\u001b[39m prepared_stopping_criteria = \u001b[38;5;28mself\u001b[39m._get_stopping_criteria(\n\u001b[32m   2370\u001b[39m     generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n\u001b[32m   2371\u001b[39m )\n\u001b[32m   2373\u001b[39m \u001b[38;5;66;03m# Set model_kwargs `use_cache` so we can use it later in forward runs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1130\u001b[39m, in \u001b[36mGenerationMixin._get_logits_processor\u001b[39m\u001b[34m(self, generation_config, input_ids_seq_length, encoder_input_ids, prefix_allowed_tokens_fn, logits_processor, device, model_kwargs, negative_prompt_ids, negative_prompt_attention_mask)\u001b[39m\n\u001b[32m   1123\u001b[39m     processors.append(\n\u001b[32m   1124\u001b[39m         ForcedBOSTokenLogitsProcessor(\n\u001b[32m   1125\u001b[39m             generation_config.forced_bos_token_id,\n\u001b[32m   1126\u001b[39m         )\n\u001b[32m   1127\u001b[39m     )\n\u001b[32m   1128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.forced_eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1129\u001b[39m     processors.append(\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[43mForcedEOSTokenLogitsProcessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforced_eos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m     )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m generation_config.remove_invalid_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     processors.append(InfNanRemoveLogitsProcessor())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\transformers\\generation\\logits_process.py:1595\u001b[39m, in \u001b[36mForcedEOSTokenLogitsProcessor.__init__\u001b[39m\u001b[34m(self, max_length, eos_token_id, device)\u001b[39m\n\u001b[32m   1593\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eos_token_id, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m   1594\u001b[39m         eos_token_id = [eos_token_id]\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m     eos_token_id = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28mself\u001b[39m.eos_token_id = eos_token_id\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_floating_point(eos_token_id) \u001b[38;5;129;01mor\u001b[39;00m (eos_token_id < \u001b[32m0\u001b[39m).any():\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import evaluate\n",
    "\n",
    "# JSON bestand inladen\n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['summary'] = sumary  # Voeg de samenvatting toe aan de DataFrame\n",
    "# Groeperen per gesprek (topic_id) en alle 'text' regels samenvoegen tot 1 tekst per gesprek\n",
    "conversations = df.groupby(\"topic_id\").agg({\n",
    "    \"text\": lambda texts: \" \".join(texts),  # alle berichten samenvoegen\n",
    "    \"summary\": \"first\"  # neem de summary (die voor alle regels gelijk is)\n",
    "}).reset_index()\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "results = []\n",
    "\n",
    "model_paths = [\n",
    "    \"./bart-summarizer\",\n",
    "    \"./long-t5-summarizer\",\n",
    "    \"./flan-t5-summarizer\",\n",
    "    \"./longformer-summarizer\",\n",
    "    \"./t5-summarizer\",\n",
    "    \"./pegasus-summarizer\",\n",
    "]\n",
    "\n",
    "for model_path in model_paths:\n",
    "    print(f\"\\nEvaluating: {model_path}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    predictions = [\n",
    "        summarizer(conversation_text, max_length=248, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "        for conversation_text in conversations[\"text\"]\n",
    "    ]\n",
    "\n",
    "    for pred in predictions[:5]:\n",
    "        print(f\"Prediction: {pred}\")\n",
    "\n",
    "    scores = rouge.compute(predictions=predictions, references=conversations[\"summary\"])\n",
    "    results.append({\n",
    "        \"model\": model_path,\n",
    "        \"rouge1\": round(scores[\"rouge1\"], 4),\n",
    "        \"rouge2\": round(scores[\"rouge2\"], 4),\n",
    "        \"rougeL\": round(scores[\"rougeL\"], 4),\n",
    "        \"rougeLsum\": round(scores[\"rougeLsum\"], 4)\n",
    "    })\n",
    "\n",
    "# Resultaten tonen\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n=== ROUGE Vergelijking ===\")\n",
    "print(df_results.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d7fdd3",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Deze code:\n",
    "- Laadt een BART-samenvattingsmodel (`bart-large-cnn-samsum`) en bijbehorende tokenizer.\n",
    "- Verwerkt JSON-chatdata per `topic_id`, combineert tekst tot Ã©Ã©n document per onderwerp.\n",
    "- Splits lange documenten op in chunks die passen in het model (minder dan `MAX_INPUT_LENGTH` tokens).\n",
    "- Genereert per chunk een samenvatting en combineert deze.\n",
    "- Slaat alles op als `.jsonl`-bestand waarin elke regel een samenvatting bevat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64bb80b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./bart-summarizer</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./long-t5-summarizer</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./flan-t5-summarizer</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./longformer-summarizer</td>\n",
       "      <td>0.4879</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>0.4337</td>\n",
       "      <td>0.4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./t5-summarizer</td>\n",
       "      <td>0.4495</td>\n",
       "      <td>0.1023</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./pegasus-summarizer</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.1090</td>\n",
       "      <td>0.1090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  rouge1  rouge2  rougeL  rougeLsum\n",
       "0        ./bart-summarizer  0.4879  0.1417  0.4337     0.4337\n",
       "1     ./long-t5-summarizer  0.0000  0.0000  0.0000     0.0000\n",
       "2     ./flan-t5-summarizer  0.4366  0.1417  0.3824     0.3824\n",
       "3  ./longformer-summarizer  0.4879  0.1417  0.4337     0.4337\n",
       "4          ./t5-summarizer  0.4495  0.1023  0.3824     0.3824\n",
       "5     ./pegasus-summarizer  0.1090  0.0217  0.1090     0.1090"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a210a98",
   "metadata": {},
   "source": [
    "**Uitleg:**  \n",
    "Hier kan je zien in de dataframe dat Bart en Longformer het beste presteren. Maar als we gaan kijken naar hoe de samenvatting er echt uit ziet. Vinden wij zelf dat die van bart er het best uit ziet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660aff65",
   "metadata": {},
   "source": [
    "Hier kijken we eerst hoe bart zonder finetunen de samenvatting zou genereren. We kunnen zien dat hij er erg veel moeite mee heeft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b9c6a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give mainly the opinions of the people: ~~Hoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullies meningen! ~~Nou, ik vind het wel een goed idea! ðŸš— Meer parkeersplekken is nooit verkeerd, toch?! ðŸ¤© ~~Ik snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden! ðŸ’ª ~~Fatimaâ€™s keuÈ™en is belangrijk. Maar eerlijk gezegd vind ik het soms echt zoeken naar een Parkeerplek als ik thuiskom van werk... ðŸ˜•@@@@@@@@Daar heb je echt samenkomen met wat\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-base\")\n",
    "\n",
    "# Laad het JSON-bestand\n",
    "with open(\"eval/eval1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "# Voeg een instructie toe voor betere focus\n",
    "input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "\n",
    "# Samenvatten met beam search en iets langere minimumlengte\n",
    "summary = summarizer(\n",
    "    input_text,\n",
    "    max_length=248,\n",
    "    min_length=50,\n",
    "    do_sample=True,\n",
    "    num_beams=4\n",
    ")[0][\"summary_text\"]\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d6729",
   "metadata": {},
   "source": [
    "# COde voor mark. Hier gebruiken we onze gefinetuned model van bart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1d48e",
   "metadata": {},
   "source": [
    "Hier vegelijken we de outputs van Longformer en Bart. Zoals je kan zien zijn het niet de beste samenvattingen. Dat komt omdat onze data niet van goede kwaliteit is. Ook hebben we niet zo veel data. En de modellen van zich zelf presteren voor het finetunen al erg matig met het samenvatten van onze chat berichten. Als we de outputs vergelijken van bart voor het finetunen en na het finetunen kunnen we wel zien dat het model iets heeft geleerd. Zo is het model na het finetunen toch wel iets beter in een samenvatting genereren dan het pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfdf860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caspe\\Jaar 3\\Datalab\\Git\\Datalab-lectoraat\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n",
      "Input ids are automatically padded from 831 to 1024 to be a multiple of `config.attention_window`: 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give me your thoughts about the parkeerterrein in de rand van het dorp.Jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar jullie meningen!Nou, ik vind het wel een goed idee! ðŸš— Meer parkeerterrein aan de rand van het dorp\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "def preprocces_json(json_file):\n",
    "    \"\"\"\n",
    "    Laad een JSON-bestand en retourneer de tekstberichten.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "    chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "    input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "    return input_text\n",
    "\n",
    "def voormark(input_text, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    summary = summarizer(\n",
    "        input_text,\n",
    "        max_length=248,\n",
    "        min_length=50,\n",
    "        do_sample=True,\n",
    "        num_beams=4\n",
    "    )[0][\"summary_text\"]\n",
    "    return summary\n",
    "input_text = preprocces_json(\"eval/eval1.json\")\n",
    "summary = voormark(input_text, \"./longformer-summarizer\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcd479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. Give mainly the opinions of the people: â€œLindaâ€, â€œHoi allemaal! Wat vinden jullie van die plannen voor een nieuw parkeerterrein aan de rand van het dorp? Goed idee of juist niet? Ben benieuwd naar Jullie meningen!â€ â€œNou, ik vind het wel een goed Idee, toch?! ðŸ¤©â€â€œIk snap het nut ervan, maar komt het niet pal naast het speelveldje? Dat zou ik echt jammer vinden van de kinderen ðŸ˜•â€œ â€œIâ€™m a man whoâ€™s a woman who is a woman in her 20s.â€ ðŸ’ªâ€œWe donâ€™t know if weâ€™re going to get to know her, but we do know that sheâ€™ll be a woman soon.â€œShe is a man in her 30s!â€œâ€œWhat if she is\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "def preprocces_json(json_file):\n",
    "    \"\"\"\n",
    "    Laad een JSON-bestand en retourneer de tekstberichten.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Combineer alle tekstberichten met nieuwe regels ertussen\n",
    "    chat_text = \"\\n\".join([msg[\"text\"] for msg in data])\n",
    "\n",
    "    input_text = \"Summarize the following conversation. Give mainly the opinions of the people:\\n\" + chat_text\n",
    "    return input_text\n",
    "\n",
    "def voormark(input_text, model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    summary = summarizer(\n",
    "        input_text,\n",
    "        max_length=248,\n",
    "        min_length=50,\n",
    "        do_sample=True,\n",
    "        num_beams=4\n",
    "    )[0][\"summary_text\"]\n",
    "    return summary\n",
    "input_text = preprocces_json(\"eval/eval1.json\")\n",
    "summary = voormark(input_text, \"./bart-summarizer\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
